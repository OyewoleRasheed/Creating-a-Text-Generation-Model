{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Text Generation Model: Process We Can Follow\nText Generation Models have various applications, such as content creation, chatbots, automated story writing, and more. They often utilize advanced Machine Learning techniques, particularly Deep Learning models like Recurrent Neural Networks (RNNs), Long Short-Term Memory Networks (LSTMs), and Transformer models like GPT (Generative Pre-trained Transformer).\n\nBelow is the process we can follow for the task of building a Text Generation Model:\n\nUnderstand what you want to achieve with the text generation model (e.g., chatbot responses, creative writing, code generation).\nConsider the style, complexity, and length of the text to be generated.\nCollect a large dataset of text that’s representative of the style and content you want to generate.\nClean the text data (remove unwanted characters, correct spellings), and preprocess it (tokenization, lowercasing, removing stop words if necessary).\nChoose a deep neural network architecture to handle sequences for text generation.\nFrame the problem as a sequence modelling task where the model learns to predict the next words in a sequence.\nUse your text data to train the model.\nFor this task, we can use the Tiny Shakespeare dataset because of two reasons:\n\n\nIt’s available in the format of dialogues, so you will learn how to generate text in the form of dialogues.\nUsually, we need huge textual datasets for building text generation models. The Tiny Shakespeare dataset is already available in the tensorflow datasets, so we don’t need to download any dataset externally.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_datasets as tfds\nimport numpy as np\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T07:43:01.570561Z","iopub.execute_input":"2025-06-04T07:43:01.570842Z","iopub.status.idle":"2025-06-04T07:43:01.575184Z","shell.execute_reply.started":"2025-06-04T07:43:01.570823Z","shell.execute_reply":"2025-06-04T07:43:01.574367Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"#Load the tiny shakespeare dataset\ndataset, info = tfds.load('tiny_shakespeare', with_info = True, as_supervised = False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T07:42:27.459734Z","iopub.execute_input":"2025-06-04T07:42:27.460457Z","iopub.status.idle":"2025-06-04T07:42:29.719627Z","shell.execute_reply.started":"2025-06-04T07:42:27.460425Z","shell.execute_reply":"2025-06-04T07:42:29.718763Z"}},"outputs":[{"name":"stdout","text":"Downloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /root/tensorflow_datasets/tiny_shakespeare/1.0.0...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Dl Completed...: 0 url [00:00, ? url/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1be66e038ce34e9d9d6c3af27a403dbc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Dl Size...: 0 MiB [00:00, ? MiB/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce657177764c4ca798d6e16d4cd14e7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train examples...: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Shuffling /root/tensorflow_datasets/tiny_shakespeare/incomplete.619TJD_1.0.0/tiny_shakespeare-train.tfrecord*.…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation examples...: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Shuffling /root/tensorflow_datasets/tiny_shakespeare/incomplete.619TJD_1.0.0/tiny_shakespeare-validation.tfrec…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test examples...: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Shuffling /root/tensorflow_datasets/tiny_shakespeare/incomplete.619TJD_1.0.0/tiny_shakespeare-test.tfrecord*..…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset tiny_shakespeare downloaded and prepared to /root/tensorflow_datasets/tiny_shakespeare/1.0.0. Subsequent calls will reuse this data.\n","output_type":"stream"},{"name":"stderr","text":"2025-06-04 07:42:29.414789: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"Our dataset contains data in a textual format. Language models need numerical data, so we’ll convert the text to sequences of integers. We’ll also create sequences for training:","metadata":{}},{"cell_type":"code","source":"#get the text from the dataset\ntext = next(iter(dataset['train']))['text'].numpy().decode('utf-8')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T07:42:35.053518Z","iopub.execute_input":"2025-06-04T07:42:35.053873Z","iopub.status.idle":"2025-06-04T07:42:35.185375Z","shell.execute_reply.started":"2025-06-04T07:42:35.053845Z","shell.execute_reply":"2025-06-04T07:42:35.184400Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"#create a mapping from unique characters to indices\n\nvocab = sorted(set(text))\nchar2idx ={char : idx for idx, char in enumerate(vocab)}\nidx2char = np.array(vocab)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T07:42:35.674353Z","iopub.execute_input":"2025-06-04T07:42:35.674753Z","iopub.status.idle":"2025-06-04T07:42:35.692355Z","shell.execute_reply.started":"2025-06-04T07:42:35.674724Z","shell.execute_reply":"2025-06-04T07:42:35.691478Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"#numerically represent the characters\ntext_as_int = np.array([char2idx[c] for c in text])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T07:42:35.900753Z","iopub.execute_input":"2025-06-04T07:42:35.901106Z","iopub.status.idle":"2025-06-04T07:42:36.030157Z","shell.execute_reply.started":"2025-06-04T07:42:35.901082Z","shell.execute_reply":"2025-06-04T07:42:36.029158Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"#create training examples and targets\nseq_length = 100\nexample_per_epoch = len(text)// (seq_length + 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T09:21:35.918823Z","iopub.execute_input":"2025-06-04T09:21:35.919519Z","iopub.status.idle":"2025-06-04T09:21:35.924118Z","shell.execute_reply.started":"2025-06-04T09:21:35.919487Z","shell.execute_reply":"2025-06-04T09:21:35.923139Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"#create training sequences\nchar_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\nsequences = char_dataset.batch(seq_length + 1,drop_remainder = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T09:21:35.958817Z","iopub.execute_input":"2025-06-04T09:21:35.959653Z","iopub.status.idle":"2025-06-04T09:21:35.976513Z","shell.execute_reply.started":"2025-06-04T09:21:35.959621Z","shell.execute_reply":"2025-06-04T09:21:35.975530Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"For each sequence, we will now duplicate and shift it to form the input and target text by using the map method to apply a simple function to each batch:","metadata":{}},{"cell_type":"code","source":"def split_input_target(chunk):\n    input_text = chunk[:-1]\n    target_text = chunk[1:]\n    return input_text, target_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T07:42:36.848123Z","iopub.execute_input":"2025-06-04T07:42:36.848404Z","iopub.status.idle":"2025-06-04T07:42:36.853281Z","shell.execute_reply.started":"2025-06-04T07:42:36.848386Z","shell.execute_reply":"2025-06-04T07:42:36.852263Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"dataset = sequences.map(split_input_target)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T07:42:37.079311Z","iopub.execute_input":"2025-06-04T07:42:37.079942Z","iopub.status.idle":"2025-06-04T07:42:37.130747Z","shell.execute_reply.started":"2025-06-04T07:42:37.079905Z","shell.execute_reply":"2025-06-04T07:42:37.129745Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"Now, we’ll shuffle the dataset and pack it into training batches:","metadata":{}},{"cell_type":"code","source":"#batch size and buffer size\nBATCH_SIZE = 64\nBUFFER_SIZE = 10000","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T07:42:37.395080Z","iopub.execute_input":"2025-06-04T07:42:37.395381Z","iopub.status.idle":"2025-06-04T07:42:37.399867Z","shell.execute_reply.started":"2025-06-04T07:42:37.395359Z","shell.execute_reply":"2025-06-04T07:42:37.398948Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"dataset = (dataset\n           .shuffle(BUFFER_SIZE)\n           .batch(BATCH_SIZE, drop_remainder = True)\n          .prefetch(tf.data.experimental.AUTOTUNE)) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T07:42:37.793825Z","iopub.execute_input":"2025-06-04T07:42:37.794156Z","iopub.status.idle":"2025-06-04T07:42:37.805217Z","shell.execute_reply.started":"2025-06-04T07:42:37.794135Z","shell.execute_reply":"2025-06-04T07:42:37.804221Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"Now, we’ll use a simple Recurrent Neural Network model with a few layers to build the model:","metadata":{}},{"cell_type":"code","source":"#length of the vocabulary\nvocab_size = len(vocab)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T07:42:38.151243Z","iopub.execute_input":"2025-06-04T07:42:38.151536Z","iopub.status.idle":"2025-06-04T07:42:38.155618Z","shell.execute_reply.started":"2025-06-04T07:42:38.151517Z","shell.execute_reply":"2025-06-04T07:42:38.154769Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"#the embedding dimension\nembedding_dim = 256","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T07:42:43.534242Z","iopub.execute_input":"2025-06-04T07:42:43.534546Z","iopub.status.idle":"2025-06-04T07:42:43.538986Z","shell.execute_reply.started":"2025-06-04T07:42:43.534522Z","shell.execute_reply":"2025-06-04T07:42:43.537753Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"#number of RNN UNITS\nrnn_units = 1024","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T07:42:43.758065Z","iopub.execute_input":"2025-06-04T07:42:43.758402Z","iopub.status.idle":"2025-06-04T07:42:43.762706Z","shell.execute_reply.started":"2025-06-04T07:42:43.758381Z","shell.execute_reply":"2025-06-04T07:42:43.761693Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Input(shape=(None,), batch_size=batch_size),  \n        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n        tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n        tf.keras.layers.Dense(vocab_size)\n    ])\n    return model\n\nmodel = build_model(vocab_size, embedding_dim,rnn_units,BATCH_SIZE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T07:42:43.986811Z","iopub.execute_input":"2025-06-04T07:42:43.987151Z","iopub.status.idle":"2025-06-04T07:42:44.229170Z","shell.execute_reply.started":"2025-06-04T07:42:43.987131Z","shell.execute_reply":"2025-06-04T07:42:44.228038Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"We’ll now choose an optimizer and a loss function to compile the model:","metadata":{}},{"cell_type":"code","source":"def loss(labels, logits):\n    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits,from_logits= True)\nmodel.compile(optimizer = 'adam',loss= loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T07:42:44.380833Z","iopub.execute_input":"2025-06-04T07:42:44.381640Z","iopub.status.idle":"2025-06-04T07:42:44.396875Z","shell.execute_reply.started":"2025-06-04T07:42:44.381605Z","shell.execute_reply":"2025-06-04T07:42:44.396018Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"We’ll now train the model:","metadata":{}},{"cell_type":"code","source":"checkpoint_dir = './training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\")\n\ncheckpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    save_weights_only=True\n)\n#train the model \nEPOCHS = 10\nhistory = model.fit(dataset, epochs = EPOCHS, callbacks = [checkpoint_callback] )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T07:43:06.518965Z","iopub.execute_input":"2025-06-04T07:43:06.519258Z","iopub.status.idle":"2025-06-04T09:21:35.587937Z","shell.execute_reply.started":"2025-06-04T07:43:06.519238Z","shell.execute_reply":"2025-06-04T09:21:35.586825Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m588s\u001b[0m 4s/step - loss: 3.1798\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m623s\u001b[0m 4s/step - loss: 2.1044\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m613s\u001b[0m 4s/step - loss: 1.8118\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m570s\u001b[0m 4s/step - loss: 1.6523\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m579s\u001b[0m 4s/step - loss: 1.5526\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m584s\u001b[0m 4s/step - loss: 1.4849\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m583s\u001b[0m 4s/step - loss: 1.4387\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m588s\u001b[0m 4s/step - loss: 1.3990\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m598s\u001b[0m 4s/step - loss: 1.3641\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m583s\u001b[0m 4s/step - loss: 1.3364\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import os\nprint(\"Files in checkpoint_dir:\", os.listdir(checkpoint_dir))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T09:31:51.994252Z","iopub.execute_input":"2025-06-04T09:31:51.994571Z","iopub.status.idle":"2025-06-04T09:31:52.000641Z","shell.execute_reply.started":"2025-06-04T09:31:51.994554Z","shell.execute_reply":"2025-06-04T09:31:51.999353Z"}},"outputs":[{"name":"stdout","text":"Files in checkpoint_dir: ['ckpt_10.weights.h5', 'ckpt_1.weights.h5', 'ckpt_6.weights.h5', 'ckpt_3.weights.h5', 'ckpt_8.weights.h5', 'ckpt_7.weights.h5', 'ckpt_4.weights.h5', 'ckpt_5.weights.h5', 'ckpt_9.weights.h5', 'ckpt_2.weights.h5']\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"After training, we can now use the model to generate text. First, we will restore the latest checkpoint and rebuild the model with a batch size of 1:","metadata":{}},{"cell_type":"code","source":"# Manually find the latest `.weights.h5` file\nckpt_files = [f for f in os.listdir(checkpoint_dir) if f.endswith('.weights.h5')]\nckpt_files.sort(key=lambda x: int(x.split('_')[1].split('.')[0]))  # Sort by checkpoint number\nlatest_ckpt = ckpt_files[-1]\nlatest_ckpt_path = os.path.join(checkpoint_dir, latest_ckpt)\n\n# Load model and weights\nmodel = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\nmodel.load_weights(latest_ckpt_path)\nmodel.build(tf.TensorShape([1, None]))\n\nprint(f\"✅ Loaded weights from: {latest_ckpt_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T09:34:28.248517Z","iopub.execute_input":"2025-06-04T09:34:28.249523Z","iopub.status.idle":"2025-06-04T09:34:28.360180Z","shell.execute_reply.started":"2025-06-04T09:34:28.249495Z","shell.execute_reply":"2025-06-04T09:34:28.359263Z"}},"outputs":[{"name":"stdout","text":"✅ Loaded weights from: ./training_checkpoints/ckpt_10.weights.h5\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"for i, layer in enumerate(model.layers):\n    print(i, layer.name, type(layer))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T09:39:51.960694Z","iopub.execute_input":"2025-06-04T09:39:51.961061Z","iopub.status.idle":"2025-06-04T09:39:51.966198Z","shell.execute_reply.started":"2025-06-04T09:39:51.961039Z","shell.execute_reply":"2025-06-04T09:39:51.965162Z"}},"outputs":[{"name":"stdout","text":"0 embedding_2 <class 'keras.src.layers.core.embedding.Embedding'>\n1 lstm_2 <class 'keras.src.layers.rnn.lstm.LSTM'>\n2 dense_2 <class 'keras.src.layers.core.dense.Dense'>\n","output_type":"stream"}],"execution_count":36},{"cell_type":"markdown","source":"Now, to generate text, we’ll input a seed string, predict the next character, and then add it back to the input, continuing this process to generate longer text:","metadata":{}},{"cell_type":"code","source":"def generate_text(model, start_string):\n    num_generate = 1000\n    input_eval = [char2idx[s] for s in start_string]\n    input_eval = tf.expand_dims(input_eval,0)\n    text_generated = []\n    model.layers[1].reset_states()\n    for i in range(num_generate):\n        predictions = model(input_eval)\n        predictions= tf.squeeze(predictions, 0)\n        predicted_id = tf.random.categorical(predictions, num_samples = 1)[-1,0].numpy()\n        input_eval = tf.expand_dims([predicted_id],0)\n        text_generated.append(idx2char[predicted_id])\n    return (start_string + ''.join(text_generated))\nprint(generate_text(model, start_string = u\"Queen: So, let's end things\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T09:44:34.095947Z","iopub.execute_input":"2025-06-04T09:44:34.096292Z","iopub.status.idle":"2025-06-04T09:44:52.445568Z","shell.execute_reply.started":"2025-06-04T09:44:34.096269Z","shell.execute_reply":"2025-06-04T09:44:52.444570Z"}},"outputs":[{"name":"stdout","text":"Queen: So, let's end things\nare this well-wars Crie than his good words,\nHaviness in so name eccuses,\nWhich on his father's a blood far ears,\nOut a' the presure of your other light\nThis doth scipt me on, strong from this far\nTriumphanent exercide beats and brother to our court!\nAnd Norfortune! Romeo! he is a devil\nTo make seems be striptany:\nI think she dy crabll himself broke\nHath cherking's prince; what, trushor a whil answer\nGold perfil in the time in a longer venta.\n\nDUKE VINCENTIO:\nKind suffer me; honour none devounds,\nWho field\nAbout him, and all the scovern\nthee winter:\nWherein time the bettee of the senators,\nA gentleman since the mountendest, I\nknock thy life thou thou would for her flims four man,\nWho were thee on abure, my batcent heart me;\nThe searing Tybalt, Petrictio, being hole\nThat but shook their own oback for you anot?\nWhief there is the pripuly of the cause.\nI must be sign means to my desire swole,\nAy, that's nothing live, one, it would Hulced Angelo;\nWh cheak and love, a good!\n\nKING RICHARD I\n","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"The generate_text function in the above code uses a trained Recurrent Neural Network model to generate a sequence of text, starting with a given seed phrase (start_string). It converts the seed phrase into a sequence of numeric indices, feeds these indices into the model, and then iteratively generates new characters, each time using the model’s most recent output as the input for the next step. This process continues for a specified number of iterations (num_generate), resulting in a stream of text that extends from the initial seed.\n\nThe function employs randomness in character selection to ensure variability in the generated text, and the final output is a concatenation of the seed phrase with the newly generated characters, typically reflecting the style and content of the training data used for the model.","metadata":{}},{"cell_type":"markdown","source":"Summary\n\n\nSo, this is how you can build a Text Generation Model with Deep Learning using Python. Text Generation Models have various applications, such as content creation, chatbots, automated story writing, and more. They often utilize advanced Machine Learning techniques, particularly Deep Learning models like Recurrent Neural Networks (RNNs), Long Short-Term Memory Networks (LSTMs), and Transformer models like GPT (Generative Pre-trained Transformer)","metadata":{}}]}